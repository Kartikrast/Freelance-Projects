{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "PH-0ReGfmX4f",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "JcMwzZxoAimU",
        "8G2x9gOozGDZ",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - School Data Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA (Exploratory Data Analysis)\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have been given two datasets:-\n",
        "- One is having the data of Schools in a country\n",
        "- Another one is having the data of the population\n",
        "\n",
        "Some of the schools are overburdened, hence we need to identify them and we also need to point out new locations where schools must be built so that a balance can be maintained between the demad of the population and the availability of schools."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Your Objective?**"
      ],
      "metadata": {
        "id": "PH-0ReGfmX4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to find the location where the burden is the most for the schools to intake the student population.\n",
        "We also need to find the locations where there is a need to build new schools"
      ],
      "metadata": {
        "id": "PhDvGCAqmjP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Knowing the Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "pd.set_option('display.max_column', 200)"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Dataset\n",
        "\n",
        "# First let us create a dataframe of the School Dataset\n",
        "school_df = pd.DataFrame(pd.read_csv('updated.csv'))\n",
        "\n",
        "# Now let us create a dataframe of the Population Dataset\n",
        "population_df = pd.DataFrame(pd.read_excel('sdmx_data_246_subset.xlsx'))\n",
        "\n",
        "# Successfully Loaded both the datasets"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "# Let's take a look at our School Dataset\n",
        "school_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's take a look at our population dataset\n",
        "population_df.head()"
      ],
      "metadata": {
        "id": "VG2njqwHS4Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "# Let's check the size of each dataset.\n",
        "school_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has 9436 rows and 7 columns"
      ],
      "metadata": {
        "id": "g9te1x_kU2gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "population_df.shape"
      ],
      "metadata": {
        "id": "J3ZVTkdOTaI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has 221 rows and 18 columns"
      ],
      "metadata": {
        "id": "b9PnDqTyU0VT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "# Let's see what all data types we are having in our datasets\n",
        "school_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great as we can see there are no null values in this dataset.\n",
        "\n",
        "This data set is having:-\n",
        "- 3 Categorical variables, and\n",
        "- 4 Numerical variables"
      ],
      "metadata": {
        "id": "UPKyDfraUvni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "population_df.info()"
      ],
      "metadata": {
        "id": "u4QxNqZOUJIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly in this dataset as well we are having no null values.\n",
        "\n",
        "This data set is having:-\n",
        "- 3 Categorical variables, and\n",
        "- 15 Numerical variables"
      ],
      "metadata": {
        "id": "hz8cQL4HUtPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "# Let's check if any of the rows in our datasets is duplicated.\n",
        "\n",
        "school_df.loc[school_df.duplicated()]\n",
        "\n",
        "# Let's check for concerned columns individually.\n",
        "# The only concerned column here is the id column.\n",
        "school_df.loc[school_df['id'].duplicated()]\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this we can see that there is no exact duplicate of any row in any column."
      ],
      "metadata": {
        "id": "9ASQ6pOPVjT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "population_df.loc[population_df.duplicated()]\n",
        "\n",
        "# Let's check for concerned columns individually.\n",
        "# The only concerned column here is the code column.\n",
        "population_df.loc[population_df['Code'].duplicated()]"
      ],
      "metadata": {
        "id": "gBue5CzsWRmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here also we can see that there is no exact duplicate of any row in any column."
      ],
      "metadata": {
        "id": "DcfI_vW_WnYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "school_df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "population_df.isna().sum()"
      ],
      "metadata": {
        "id": "2I9V_UhxXzU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we have already checked for the missing values we still need to check if there is any 0 values in any column where it shouldn't be there."
      ],
      "metadata": {
        "id": "RmG11lVGX37Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first check with school data.\n",
        "\n",
        "school_df.head()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this we can see the 'projected_volume' and the 'shifts' columns cannot have 0 as their value.\n",
        "\n",
        "Let's check if any of these columns have any 0 values."
      ],
      "metadata": {
        "id": "tOZA94zDZC4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for projected_volume column\n",
        "school_df.loc[school_df['projected_volume']==0]"
      ],
      "metadata": {
        "id": "CLZSh16EZN7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see here are few rows that are having 0 as their value."
      ],
      "metadata": {
        "id": "ALbuITt1ZsBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for shifts column\n",
        "school_df.loc[school_df['shifts']==0]"
      ],
      "metadata": {
        "id": "GaG3IvqBZy4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great we can see there are no 0 values in 'shifts' column."
      ],
      "metadata": {
        "id": "yMtSNNXbZ75C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's check of population dataset.\n",
        "population_df.head()"
      ],
      "metadata": {
        "id": "22Jx2quIaUKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see none of the year columns can have 0 population.\n",
        "\n",
        "Let's check if any of these columns have any 0 values."
      ],
      "metadata": {
        "id": "gQAnG8k9ajkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "population_df.describe()"
      ],
      "metadata": {
        "id": "mL72cgxObQcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the minimum value in all the columns except for 2023 is have 0. Hence we will be handling these values accordingly."
      ],
      "metadata": {
        "id": "G5DLQBbWcApS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let us handle the values in School Data.\n",
        "school_df.loc[school_df['projected_volume']==0]"
      ],
      "metadata": {
        "id": "5d_8wSZOdbFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As we can see there are only 3 districts in concern - ('Chiroqchi tumani', 'Shaxrisabz tumani' and 'Kattaqo+IBg-rg+IBg-on shaxar')\n",
        "# Let's check them individually\n",
        "school_df.loc[school_df['district']=='Chiroqchi tumani']\n",
        "\n",
        "# As there are many values we will check with the concerned \"mahalla\"\n",
        "school_df.loc[school_df['mahalla']=='Dustlik diyori MFY']\n",
        "\n",
        "# Here we can see we can 2 same rows however the id and the projected_volume are different.\n",
        "# Hence in this case it is safe to drop the row with 0 projected_volume"
      ],
      "metadata": {
        "id": "nni8HGagd118"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That index has been dropped however the code has been removed to avoid any mistake."
      ],
      "metadata": {
        "id": "nyWeaeuWh29z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's reset the index.\n",
        "school_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "4tXV1zzPgw7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for others\n",
        "school_df.loc[school_df['mahalla']=='Langar MFY']\n",
        "\n",
        "# In this case also we can drop the 0 values in order to maintain accuracy of our analysis.\n",
        "\n",
        "# That index has been dropped however the code has been removed to avoid any mistake."
      ],
      "metadata": {
        "id": "gc1dEShhhOpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "school_df.loc[school_df['mahalla']=='Langar MFY']\n",
        "\n",
        "# Let's check for others\n",
        "school_df.loc[school_df['mahalla']=='Arabbandi MFY']\n",
        "\n",
        "# That index has been dropped however the code has been removed to avoid any mistake.\n",
        "\n",
        "# Let's check for others\n",
        "school_df.loc[school_df['mahalla']=='Xisor MFY']\n",
        "\n",
        "# That index has been dropped however the code has been removed to avoid any mistake.\n",
        "school_df.loc[school_df['mahalla']=='Xisor MFY']\n",
        "\n",
        "# Let's check for others\n",
        "school_df.loc[school_df['mahalla']==\"Kattaqo+IBg-rg'on MFY\"]\n",
        "# This one is having only 1 row, hence let's check for the district\n",
        "\n",
        "school_df.loc[(school_df['district']=='Kattaqo+IBg-rg+IBg-on shaxar') & (school_df['actual_students'] <= 1000)]\n",
        "\n",
        "# Here we can see the actual students are 805, so in order to keep it competetive we will replace it with 900\n",
        "school_df['projected_volume'].replace(0, 900, inplace=True)\n",
        "\n",
        "school_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "jYIUtx29iN6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the 0 values have been handled carefully."
      ],
      "metadata": {
        "id": "USUkrB6yppaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the population data we will handle the 0s while working with the data so that it can be handled accordingly."
      ],
      "metadata": {
        "id": "XVQou6BnrBjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all after throughly studying the given datasets here are few pointers that I am able to seek from the Data sets:-\n",
        "\n",
        "- School Data:-\n",
        "  - There were no null values in the dataset which means the given dataset was already pretty much clean and sophisticated.\n",
        "  - The dataset was having 3 categorical and 4 numerical variable columns.\n",
        "  - There were no exact duplicates of any rows, however there were few rows with 0 values which were carefully detected and handled.\n",
        "  - There are multiple groups as per the region, district, and mahalla. The region can work as the universal set in which the hirarchy of the dictrict followed by mahallas can be seen.\n",
        "  - The relationship of demand and supply in these areas can be interpreted by the values in projected_volume and actual_students columns.\n",
        "\n",
        "- Population Data:-\n",
        "  - In this dataset as well there were no null values, hence it was also a clean dataset.\n",
        "  - This dataset was having 3 categorical values from which only 1 is actually required, the other 15 are the numerical values from which 14 are the population over the years.\n",
        "  - In this data set as well there were no exact duplicates however there are some 0 values that are to be taken care of accordingly.\n",
        "  - Using the columns with the population we will be able to predict the expected population growth of any place and hence we will be able to frame our decisions regarding buliding schools more informatively."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "school_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "population_df.columns"
      ],
      "metadata": {
        "id": "3apFQmSJvqQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In the population dataset we can drop the 'Klassifikator' and 'Klassifikator_ru' columns as we only need to work with EN one.\n",
        "population_df = population_df[['Code', 'Klassifikator_en', '2010',\n",
        "       '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019',\n",
        "       '2020', '2021', '2022', '2023']].copy()\n"
      ],
      "metadata": {
        "id": "aDi81o8Wvz-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "school_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "population_df.describe()"
      ],
      "metadata": {
        "id": "4pGX_kBlycJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "School Data:-\n",
        "  - The projected_count and the actual_students are the most important numerical variables in this dataset.\n",
        "  - We can clearly see that there are few outliers that are there in both of these columns and we need to handle them accordingly.\n",
        "\n",
        "Population Data:-\n",
        "  - The population growth over the years is the most effective information that we can derive from these variables.\n",
        "  - The latest population that we are having we give us a good idea on the need of schools as per the demand."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's check unique values in School Data\n",
        "school_df['region'].unique()\n",
        "\n",
        "# Let's check how many unique regions are there\n",
        "len(school_df['region'].unique())"
      ],
      "metadata": {
        "id": "r02IsFXTQMSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***\n",
        "\n"
      ],
      "metadata": {
        "id": "e_VbB9zqfosu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping data together as per regions\n",
        "region_groups = school_df.groupby('region')\n",
        "\n",
        "regions = []\n",
        "region_capacity = []\n",
        "region_students = []\n",
        "region_shifts = []\n",
        "\n",
        "for region, data in region_groups:\n",
        "  regions.append(region)\n",
        "  region_capacity.append(data['projected_volume'].sum())\n",
        "  region_students.append(data['actual_students'].sum())\n",
        "  region_shifts.append(data['shifts'].sum()/len(data['shifts']))\n",
        "\n",
        "region_df = pd.DataFrame({\n",
        "    'region': regions,\n",
        "    'Capacity': region_capacity,\n",
        "    'Students': region_students,\n",
        "    'Shifts': region_shifts\n",
        "})\n",
        "\n",
        "# Let's create a new column in region_df which tells us students per shift\n",
        "region_df['Stud/Shift'] = region_df['Students']//(region_df['Shifts'])\n",
        "\n",
        "# Let's now create a column to check the  difference b/w capacity and students.\n",
        "region_df['ratio'] = round(region_df['Capacity']/region_df['Stud/Shift'], 1)\n",
        "\n",
        "# Let's sort the data to get a better idea\n",
        "sorted_region_df = region_df.sort_values(by='ratio').reset_index(drop=True)\n",
        "\n",
        "sorted_region_df"
      ],
      "metadata": {
        "id": "8ywOl9m4AmZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can clearly see that on a broader level the regions have enough capacity to intake students, however if we narrow down this observation, the out come may vary, which shows that the capacity distribution is biased. Let us have a look at districts."
      ],
      "metadata": {
        "id": "BV3_Gd80GhLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping data together as per districts\n",
        "dist_groups = school_df.groupby('district')\n",
        "\n",
        "districts = []\n",
        "dist_capacity = []\n",
        "dist_students = []\n",
        "dist_shifts = []\n",
        "\n",
        "for dist, data in dist_groups:\n",
        "  districts.append(dist)\n",
        "  dist_capacity.append(data['projected_volume'].sum())\n",
        "  dist_students.append(data['actual_students'].sum())\n",
        "  dist_shifts.append(data['shifts'].sum()/len(data['shifts']))\n",
        "\n",
        "dist_df = pd.DataFrame({\n",
        "    'Districts': districts,\n",
        "    'Capacity': dist_capacity,\n",
        "    'Students': dist_students,\n",
        "    'Shifts': dist_shifts\n",
        "})\n",
        "\n",
        "# Let's create a new column in region_df which tells us students per shift\n",
        "dist_df['Stud/Shift'] = dist_df['Students']//(dist_df['Shifts'])\n",
        "\n",
        "# Let's now create a column to check the  difference b/w capacity and students.\n",
        "dist_df['ratio'] = round(dist_df['Capacity']/dist_df['Stud/Shift'], 1)\n",
        "\n",
        "# Let's sort the data to get a better idea\n",
        "sorted_dist_df = dist_df.sort_values(by='ratio').reset_index(drop=True)\n",
        "\n",
        "sorted_dist_df"
      ],
      "metadata": {
        "id": "QMA_Z9QzHEdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that the Bekabod city\tis having negative difference which means that this city need to increase the intake capacity. Let us now check this as per mahalla column."
      ],
      "metadata": {
        "id": "VxLMu62IIuHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping data together as per mahalla\n",
        "mahalla_groups = school_df.groupby('mahalla')\n",
        "\n",
        "mahallas = []\n",
        "mah_capacity = []\n",
        "mah_students = []\n",
        "mah_shifts = []\n",
        "\n",
        "for mah, data in mahalla_groups:\n",
        "  mahallas.append(mah)\n",
        "  mah_capacity.append(data['projected_volume'].sum())\n",
        "  mah_students.append(data['actual_students'].sum())\n",
        "  mah_shifts.append(data['shifts'].sum()/len(data['shifts']))\n",
        "\n",
        "mah_df = pd.DataFrame({\n",
        "    'Mahalla': mahallas,\n",
        "    'Capacity': mah_capacity,\n",
        "    'Students': mah_students,\n",
        "    'Shifts': mah_shifts\n",
        "})\n",
        "\n",
        "# Let us handle outliers considering any value in Capacity which is less than 100\n",
        "outliers = mah_df['Capacity'] < 100\n",
        "\n",
        "mah_df_no_outliers = mah_df.mask(outliers)\n",
        "\n",
        "# Let's create a new column in region_df which tells us students per shift\n",
        "mah_df_no_outliers['Stud/Shift'] = mah_df_no_outliers['Students']//(mah_df_no_outliers['Shifts'])\n",
        "\n",
        "# Let's now create a column to check the  difference b/w capacity and students.\n",
        "mah_df_no_outliers['ratio'] = round(mah_df_no_outliers['Capacity']/mah_df_no_outliers['Stud/Shift'], 2)\n",
        "\n",
        "# Let's sort the data to get a better idea\n",
        "sorted_mah_df = mah_df_no_outliers.sort_values(by='ratio').reset_index(drop=True)\n",
        "\n",
        "# Let's check all the values where difference <= -100\n",
        "print(sorted_mah_df.loc[sorted_mah_df['ratio']<1].shape) # to count the no. of rows\n",
        "map_df = sorted_mah_df.loc[sorted_mah_df['ratio']<1]"
      ],
      "metadata": {
        "id": "ZFdTIvUoJMll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can clearly see the biasness in the capacity to intake students as these are the obvious places where is an absolute need to create schools, as these locations are over burdened, however we can throughly check what all can be seen further."
      ],
      "metadata": {
        "id": "co50pwemK0AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now work on the population data and see what all information we can derive from it."
      ],
      "metadata": {
        "id": "4-asrkCbLoIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at population_df\n",
        "population_df.head()\n",
        "\n",
        "# Let's check NA values\n",
        "population_df.isna().sum() # No NA values\n",
        "\n",
        "# We do have some 0s in the year columns however they are manageable as we are having the latest record\n",
        "# First we will change those 0s into NaN values\n",
        "\n",
        "population_df = population_df.replace(0, np.nan).copy()\n",
        "population_df.describe() # No 0 values\n",
        "\n",
        "# We will now create a new column in the dataframe to check the Population Growth Rate\n",
        "population_df['Growth Rate'] = round((((population_df['2023'] - population_df['2010'])/population_df['2010'])/13)*100, 1)\n",
        "\n",
        "# Now we will change these NA values back to 0\n",
        "population_df = population_df.replace(np.nan, 0).copy()\n",
        "\n",
        "population_df.head()\n"
      ],
      "metadata": {
        "id": "39gBgkIaL0f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check for the highest growth rate\n",
        "sorted_growth_pop = population_df.sort_values(by='Growth Rate', ascending=False).reset_index(drop=True)\n",
        "sorted_growth_pop.head(10) # These are the top 10 places where the population growth was highest"
      ],
      "metadata": {
        "id": "0Cvwm28YUBmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's rename the 'klassifikator_en' column so that we can merge the data frames\n",
        "sorted_growth_pop.rename(columns={'Klassifikator_en': 'district'}, inplace=True)\n",
        "\n",
        "\n",
        "# Let us now create a new data frame to merge our 2 datasets.\n",
        "dist_name_groups = school_df.groupby('district')\n",
        "\n",
        "district_names = []\n",
        "name_capacity = []\n",
        "name_students = []\n",
        "shifts = []\n",
        "\n",
        "for name, data in dist_name_groups:\n",
        "  district_names.append(name)\n",
        "  name_capacity.append(data['projected_volume'].sum())\n",
        "  name_students.append(data['actual_students'].sum())\n",
        "  shifts.append(round(data['shifts'].sum()/len(data['shifts']), 2))\n",
        "\n",
        "dist_df = pd.DataFrame({\n",
        "    'district': district_names,\n",
        "    'Capacity': name_capacity,\n",
        "    'Students': name_students,\n",
        "    'Shifts': shifts\n",
        "})\n",
        "\n",
        "merged_df = pd.merge(dist_df, sorted_growth_pop, on='district')\n",
        "\n",
        "# Let's sort the values\n",
        "merged_df = merged_df.sort_values(by=\"Growth Rate\", ascending=False).reset_index(drop=True)\n",
        "merged_df.head()\n",
        "\n",
        "merged_df['Stud/Shift'] = merged_df['Students']//(merged_df['Shifts'])\n",
        "\n",
        "# We need to create the difference column to see the status\n",
        "merged_df['ratio'] = round(merged_df['Capacity']/merged_df['Stud/Shift'], 2)\n",
        "\n",
        "merged_df.head()\n",
        "\n",
        "# Let's sort them on the basis of Difference\n",
        "sorted_ratio_merged = merged_df.sort_values(by='ratio').reset_index(drop=True)\n",
        "\n",
        "sorted_ratio_merged"
      ],
      "metadata": {
        "id": "524jJ3b1Md-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will only be having those values that are common in both the datasets."
      ],
      "metadata": {
        "id": "tnw9cfGjzibP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's estimate what will be the population in the coming year as per the current growth rate\n",
        "sorted_growth_pop['2024'] = sorted_growth_pop['2023'] + (sorted_growth_pop['2023']*sorted_growth_pop['Growth Rate'])/100\n",
        "\n",
        "sorted_growth_pop"
      ],
      "metadata": {
        "id": "K_6Nfw3bx9BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the districts with the highest population currently\n",
        "highest_pop = sorted_growth_pop.sort_values(by='2023', ascending=False).reset_index(drop=True)\n",
        "highest_pop.iloc[1:].head()"
      ],
      "metadata": {
        "id": "6g0TCp1tshY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's merge the data frames on region\n",
        "sorted_growth_pop.rename(columns={'district':'region'}, inplace=True)\n",
        "merged_region_df = pd.merge(sorted_region_df, sorted_growth_pop, on='region')\n",
        "\n",
        "sorted_merged_region = merged_region_df.sort_values(by='2023', ascending=False).reset_index(drop=True)\n",
        "sorted_merged_region\n",
        "\n",
        "sorted_merged_region['Stud/Shift'] = sorted_merged_region['Students']//(sorted_merged_region['Shifts'])\n",
        "\n",
        "# We need to create the difference column to see the status\n",
        "sorted_merged_region['ratio'] = round(sorted_merged_region['Capacity']/sorted_merged_region['Stud/Shift'], 2)\n",
        "\n",
        "sorted_merged_region.head()\n",
        "\n",
        "# Let's sort them on the basis of Difference\n",
        "sorted_ratio_merged_region = sorted_merged_region.sort_values(by='ratio').reset_index(drop=True)\n",
        "\n",
        "sorted_ratio_merged_region"
      ],
      "metadata": {
        "id": "Dj77eJButSpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data Manipulation is typically divided into 3 parts:-\n",
        "- Manipulation as per the School Data\n",
        "- Manipulation as per the Population Data\n",
        "- Manipulation as per the Merged Data\n",
        "\n",
        "1. School Data:-\n",
        "- First we checked for the regions where the capacity and students per shift ratio was the least. sorted_region_df\n",
        "\n",
        "- Then we checked for the districts where the capacity and students per shift ratio was the least. sorted_dist_df\n",
        "\n",
        "- Then we finally checked for the mahallas where the capacity and students per shift ratio was the least. sorted_mah_df\n",
        "\n",
        "With the mahallas study we were able to check the major locations where there was a need to build schools, we handled the outliers in the data so that the accuracy can be maintained.\n",
        "\n",
        "sorted_mah_df dataframe tell us where there is a need to build a school.\n",
        "Not only that if we will be checking the tail of these dataframes we will be able to see which schools are underutilized which is increasing the burden on other schools.\n",
        "\n",
        "Population Data:-\n",
        "- First we created new column in the data set to check the population growth rate. sorted_growth_pop.head(10): These are the top 10 places where the population growth was highest.\n",
        "- Then we created another new column to check the estimated population of that location by 2024 as per the growth rate.\n",
        "- Then we checked for the districts with the current highest population. highest_pop.iloc[2:].head(10) shows the top 10 highest populated locations.\n",
        "\n",
        "Merged Data:-\n",
        "- We merged the data on 2 basis: 'region' and 'district'\n",
        "- In this district based merge we checked for the ration of capacity per student. sorted_ratio_merged: Shows the data.\n",
        "- We checked the same thing in region based merge also. sorted_ratio_merged_region: shows the least ratio with capacity per student."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Let's check for the underutilization of schools as per Mahallas\n",
        "\n",
        "sorted_mah_df = sorted_mah_df.dropna()\n",
        "sorted_mah_df.tail(10)\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "plt.title('10 most underutilized Mahallas')\n",
        "sns.barplot(x=sorted_mah_df['Mahalla'].tail(10), y=sorted_mah_df['ratio'].tail(10))\n",
        "\n",
        "for x, y in zip(sorted_mah_df['Mahalla'].tail(10), sorted_mah_df['ratio'].tail(10)):\n",
        "  plt.text(x, y, f\"{y}\", ha='center', va='bottom')\n",
        "\n",
        "plt.savefig('/content/drive/My Drive/ch1_plot.png')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a Bar plot can easily indicate the intensity of the locations which are underutilized as per the capacity per student ratio."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Aloqali is the region with the highest under utilization, here the capacity is 18.35 seats for each student. Others can also be seen from the chart."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Let us now check for the opposite\n",
        "plt.figure(figsize=(18, 5))\n",
        "plt.title('Top 10 locations where there is a need for built')\n",
        "sns.barplot(x=sorted_mah_df['Mahalla'].head(10), y=sorted_mah_df['ratio'].head(10))\n",
        "\n",
        "for x, y in zip(sorted_mah_df['Mahalla'].head(10), sorted_mah_df['ratio'].head(10)):\n",
        "  plt.text(x, y, f\"{y}\", ha='center', va='bottom')\n",
        "\n",
        "plt.savefig('/content/drive/My Drive/ch2_plot.png')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar plot clearly shows those Mahallas where there is a need to built the schools, as the capacity is too less as compared to the student population."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here we can see that the Orol MFY is in the most need for this, however this cannot be the only factor on which the decision can be made as we also need to check with the community and the nearby area if they can fulfill this need."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_growth_pop.head()"
      ],
      "metadata": {
        "id": "C0nuKrBt1Af0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Population growth rate over time for the top 10 locations with the highest population growth.\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "plt.title(\"Population growth rate over time for the top 10 locations with the highest population growth\")\n",
        "plt.xlabel('Regions')\n",
        "plt.ylabel('Growth Rate')\n",
        "sns.scatterplot(x='region', y='Growth Rate', data=sorted_growth_pop.head(10), color='blue', legend=False, marker='o')\n",
        "plt.plot(sorted_growth_pop['region'].head(10), sorted_growth_pop['Growth Rate'].head(10), color='black')\n",
        "\n",
        "for x, y in zip(sorted_growth_pop['region'].head(10), sorted_growth_pop['Growth Rate'].head(10)):\n",
        "  plt.text(x, y, f\"{y}\", ha='center', va='bottom')\n",
        "plt.savefig('/content/drive/My Drive/ch3_plot.png')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line plot can easily show the graphical status of the growth rate in population as per the regions over the time."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the Bektemir District is having the highest growth along with the others."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Now let us locate the districts that are there in the population growth and seek the status of their capacity\n",
        "growth_rate_regions = []\n",
        "for region in sorted_growth_pop['region'].head(10):\n",
        "  growth_rate_regions.append(region)\n",
        "\n",
        "\n",
        "growth_merged_df = merged_df.loc[merged_df['district'].isin(growth_rate_regions)]\n",
        "plt.figure(figsize=(18, 5))\n",
        "plt.title(\"Capacity Ratio of top districts with highest growth rate\")\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Ratio')\n",
        "sns.scatterplot(x='district', y='ratio', data=growth_merged_df, color='blue', legend=False, marker='o')\n",
        "plt.plot(growth_merged_df['district'], growth_merged_df['ratio'], color='black')\n",
        "\n",
        "for x, y in zip(growth_merged_df['district'], growth_merged_df['ratio']):\n",
        "  plt.text(x, y, f\"{y}\", ha='center', va='bottom')\n",
        "\n",
        "plt.savefig('/content/drive/My Drive/ch4_plot.png')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the line plot it is clear to see the ratios in which the capacity per student can be visulized."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These districts can be the center of focus where the burden is quiet and the requirement to build schools is much higher. As the merged df only contains the column values that are common in both the datasets, the other values have been dropped however they are still accessable in their respective dataframes."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Top 10 districts with least capacity and students relationship\n",
        "sorted_dist_df.head(10)\n",
        "\n",
        "plt.title('Capacity vs Students')\n",
        "sns.scatterplot(data=sorted_dist_df, x='Capacity', y='Stud/Shift', hue='Districts', legend=False, s=32, alpha=0.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "plt.savefig('/content/drive/My Drive/ch5_plot.png')\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plot is the best way to reflect relation between any 2 numerical values."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that the student capacity distribution is about linear however in some places there is an utmost need to handle the distribution and balance, this can be filtered with the combination of checking with the mah_df that which regions are mostly required to be in focus."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "35rJu-qnQ7X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "\n",
        "\n",
        "def save_file(url, file_name):\n",
        "  r = requests.get(url)\n",
        "  with open(file_name, 'wb') as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "\n",
        "save_file('https://courses.cs.washington.edu/courses/cse163/19sp/' +\n",
        "          'files/lectures/05-13/data.zip', 'data.zip')\n",
        "save_file('https://courses.cs.washington.edu/courses/cse163/19sp/' +\n",
        "          'files/lectures/05-13/gz_2010_us_040_00_5m.json',\n",
        "          'gz_2010_us_040_00_5m.json')\n",
        "save_file('https://courses.cs.washington.edu/courses/cse163/19sp/' +\n",
        "          'files/lectures/05-13/stormhistory.csv', 'stormhistory.csv')\n",
        "\n",
        "with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "!pip install --upgrade geopandas\n",
        "!pip install --upgrade pyshp\n",
        "!pip install --upgrade shapely\n",
        "!pip install --upgrade descartes"
      ],
      "metadata": {
        "id": "ZRQCMSwjRFBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd"
      ],
      "metadata": {
        "id": "5FfOYqT7dD4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_mah_df.describe()"
      ],
      "metadata": {
        "id": "aamZkpe3dj-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can clearly see here that the std is too high, which means that the data is widly spreaded and is biased.\n",
        "However the average suggests that the ratio is positive on an average hence in this case we can only focus on those areas where the ratio is negative."
      ],
      "metadata": {
        "id": "tmddJtqLjn2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column to check students per shift\n",
        "school_df['Stud/Shift'] = school_df['actual_students']//school_df['shifts']\n",
        "\n",
        "# Creating a new column to check the capacity ratio\n",
        "school_df['Ratio'] = school_df['projected_volume']/school_df['Stud/Shift']\n",
        "\n",
        "overburden_ratio = school_df.loc[school_df['Ratio'] < 1]\n",
        "sorted_overburden_ratio = overburden_ratio.sort_values(by='Ratio').reset_index(drop=True)\n",
        "\n",
        "sorted_overburden_ratio['Difference'] = sorted_overburden_ratio['projected_volume'] - sorted_overburden_ratio['Stud/Shift']\n",
        "\n",
        "# Handling Outliers | Taking 50 as measure\n",
        "sorted_overburden_ratio.loc[sorted_overburden_ratio['projected_volume'] < 50]\n",
        "\n",
        "# Dropping the outliers\n",
        "sorted_overburden_ratio = sorted_overburden_ratio.drop(sorted_overburden_ratio[sorted_overburden_ratio['projected_volume'] < 50].index)\n",
        "\n",
        "sorted_overburden_ratio.reset_index(drop=True)\n",
        "\n",
        "district_ratio_groups = sorted_overburden_ratio.groupby('district')\n",
        "\n",
        "district_groups = []\n",
        "ratios = []\n",
        "for district, data in district_ratio_groups:\n",
        "  district_groups.append(district)\n",
        "  ratios.append(data['Ratio'].mean())\n",
        "\n",
        "district_ratios = pd.DataFrame({\n",
        "    'District': district_groups,\n",
        "    'Ratio': ratios\n",
        "})\n",
        "district_ratios"
      ],
      "metadata": {
        "id": "cMzPN35Okaje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import Normalize\n",
        "from matplotlib.cm import ScalarMappable\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "norm = Normalize(vmin=district_ratios['Ratio'].min(), vmax=district_ratios['Ratio'].max())\n",
        "district_ratios['normalized_ratio'] = norm(district_ratios['Ratio'])\n",
        "\n",
        "\n",
        "shapefile = '/content/district.shp'\n",
        "districts = gpd.read_file(shapefile)\n",
        "\n",
        "\n",
        "district_ratio = district_ratios.set_index('District')\n",
        "districts = districts.join(district_ratios)\n",
        "districts = districts.dropna()\n",
        "colors = [(0.0, '#FF0000'), (0.2, '#FF3131'), (0.4, '#FF5B5B'), (0.6, '#FF6E6E'), (0.8, '#FF936E'), (1.0, '#FF953D')]\n",
        "cmap = LinearSegmentedColormap.from_list('custom_cmap', colors)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(25, 25))\n",
        "districts.plot(ax=ax, column='normalized_ratio', cmap=cmap, edgecolor='black', legend=False)\n",
        "\n",
        "# sm = ScalarMappable(norm=norm, cmap=cmap)\n",
        "# cbar = fig.colorbar(sm)\n",
        "# cbar.set_label('Normalized Ratio')\n",
        "\n",
        "# for idx, row in districts.iterrows():\n",
        "#     ax.annotate(text=row['District'], xy=row['geometry'].centroid.coords[0], color='black', fontsize=8, ha='center')\n",
        "plt.title('Districts by Normalized Ratio')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.savefig('map1_plot.png')\n"
      ],
      "metadata": {
        "id": "yPQr21MTpsSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import load_workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "\n",
        "output_excel_file = 'Output.xlsx'\n",
        "workbook = load_workbook(output_excel_file)\n",
        "plot_image = Image('map_plot.png')\n",
        "worksheet = workbook['Map']\n",
        "worksheet.add_image(plot_image, 'A10')\n",
        "workbook.save(output_excel_file)"
      ],
      "metadata": {
        "id": "5aSAcED1xV1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating file"
      ],
      "metadata": {
        "id": "9mtxPbgShjqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openpyxl\n",
        "with pd.ExcelWriter('output.xlsx') as writer:\n",
        "    # Write each DataFrame to a separate worksheet\n",
        "    school_df.to_excel(writer, sheet_name='School Data')\n",
        "    population_df.to_excel(writer, sheet_name='Population Data')\n",
        "    sorted_region_df.to_excel(writer, sheet_name='Regional Data')\n",
        "    sorted_dist_df.to_excel(writer, sheet_name='District Data')\n",
        "    sorted_mah_df.to_excel(writer, sheet_name='Mahalla Data')\n",
        "    sorted_ratio_merged.to_excel(writer, sheet_name='Mergerd by District')\n",
        "    sorted_growth_pop.to_excel(writer, sheet_name='Grwoth Rate')\n",
        "    highest_pop.to_excel(writer, sheet_name='Sorted Population')\n",
        "    sorted_ratio_merged_region.to_excel(writer, sheet_name='Merged by Region')"
      ],
      "metadata": {
        "id": "z4Iwli_YhhSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import load_workbook  # Import load_workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "output_excel_file = '/content/output.xlsx'\n",
        "plots = ['/content/drive/MyDrive/ch1_plot.png', '/content/drive/MyDrive/ch2_plot.png', '/content/drive/MyDrive/ch3_plot.png', '/content/drive/MyDrive/ch4_plot.png', '/content/drive/MyDrive/ch5_plot.png']\n",
        "workbook = load_workbook(output_excel_file)\n",
        "for i, plot_file in enumerate(plots):\n",
        "    worksheet_name = f'Plot_{i+1}'\n",
        "    worksheet = workbook.create_sheet(title=worksheet_name)\n",
        "    img = Image(plot_file)\n",
        "    worksheet.add_image(img, 'A1')\n",
        "workbook.save(output_excel_file)"
      ],
      "metadata": {
        "id": "HULR8Po01cJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Solution to Objective**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JcMwzZxoAimU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What do you suggest the client to achieve Business Objective ?\n",
        "Explain Briefly."
      ],
      "metadata": {
        "id": "8G2x9gOozGDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "pASKb0qOza21"
      }
    }
  ]
}